{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GENERATOR PART - Python 3.12\n",
    "```python3 -m venv venv\n",
    "\\venv\\Scripts\\Activate.ps1 # for Windows\n",
    "pip install -r requirements.txt\n",
    "pip freeze > requirements.txt\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLAMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Zuza\\MAGISTERKA\\AI-Generated-CTI\\venv\\Lib\\site-packages\\unsloth_zoo\\gradient_checkpointing.py:330: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\c10/cuda/CUDAAllocatorConfig.h:28.)\n",
      "  GPU_BUFFERS = tuple([torch.empty(2*256*2048, dtype = dtype, device = f\"cuda:{i}\") for i in range(n_gpus)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.4.8: Fast Llama patching. Transformers: 4.51.3.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 5080. Num GPUs = 1. Max memory: 15.92 GB. Platform: Windows.\n",
      "O^O/ \\_/ \\    Torch: 2.7.0+cu128. CUDA: 12.0. CUDA Toolkit: 12.8. Triton: 3.3.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.4.8 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n",
      "Unsloth: Already have LoRA adapters! We shall skip this step.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "\n",
    "model_name = \"Meta-Llama-3.1-8B\"\n",
    "\n",
    "max_seq_length = 2048\n",
    "dtype = None  # TODO: try torch.float16 or torch.bfloat16\n",
    "load_in_4bit = True  # enable 4-bit quantization to reduce memory usage\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=model_name, # if want to download: model_name=f\"unsloth/{model_name}\"\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    ")\n",
    "\n",
    "# settings from unsloth\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=16,            # use one of 8, 16, 32, 64, 128\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0,  # 0 is optimized\n",
    "    bias=\"none\",     # \"none\" is optimized\n",
    "    use_gradient_checkpointing=\"unsloth\",  # \"unsloth\" for long context\n",
    "    random_state=3407,\n",
    "    use_rslora=False,\n",
    "    loftq_config=None,\n",
    ")\n",
    "\n",
    "# save the model\n",
    "# model.save_pretrained(model_name)\n",
    "# tokenizer.save_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token\n",
    "def format_prompt(data):\n",
    "    instructions = data[\"instruction\"]\n",
    "    inputs = data[\"input\"]\n",
    "    outputs = data[\"output\"]\n",
    "    texts = []\n",
    "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
    "        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return {\"text\": texts, }\n",
    "\n",
    "\n",
    "# llama is using alpaca prompt\n",
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files=\"aptnotes_dataset.jsonl\", split=\"train\")\n",
    "dataset = dataset.map(format_prompt, batched=True)\n",
    "\n",
    "dataset = dataset.train_test_split(test_size=0.1, shuffle=True, seed=42)\n",
    "train_dataset = dataset[\"train\"]\n",
    "eval_dataset = dataset[\"test\"]\n",
    "\n",
    "# TODO: adjust train test split!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nAnalyse the following APT report\\n\\n### Input:\\n\\n\\n### Response:\\nanalyzing closely your requirements and the physical and cyber environments in\\nwhich you must operate, Arcanum Global s holistic team of technical, operational\\nand management specialists will recommend specific (and potentially sensitive)\\nsolutions and then stand beside you to implement them and assure you realize\\nyour goals and achieve mission success.\\nEmails published by Kazaword and analyzed by Mediapart allege that Arcanum\\nemployed Bernard Squarcini, head of France s domestic intelligence agency, the\\nDirection centrale du renseignement int rieur (DCRI) from 2007 to 2012, to inform the\\nKazakh authorities of the progress of the legal proceedings against Ablyazov and to lobby\\ncertain figures in France. Squarcini confirmed to Mediapart that the government of\\nKazakhstan is a client, but Arcanum spokeswoman Yael Hartmann denied that the\\ncompany was responsible for the spearphishing attempts, insisting that the company has\\ncomplied with Swiss law.\\nThere is certainly some strong evidence consistent with there being a link between\\nOperation Manul and the government of Kazakhstan and between the government of\\nKazakhstan and Arcanum. However, we observe no direct links between Operation\\nManul and Arcanum. The technical evidence discussed below, we believe points instead\\nto an Indian company: Appin.\\nObserved Links To Appin\\nWe examined the behavior of the command and control domains used by Operation\\nManul as they moved from IP to IP.\\nUsing Passive Total, we observed that the C2\\ndomains from Operational Manul used a total of 76 IPs from 2008-07-20 to 2016-05-11.\\nWe must consider that these domains could have been used by other actors over this time\\nperiod.\\nWhile considering attribution of the actors behind Operation Manul, we investigated the\\npossibility of infrastructure overlap with known actors. Gathering data from existing\\nAPT reports we automated gathering of historical data from known APT domains from\\nthe Passive Total API and comparison with the historical data from Operational Manul\\ndomains.\\nFrom this we were able to observe overlaps between Operation Manul and an actor\\nknown as Appin. Appin is an Indian company that allegedly provides offensive\\nELECTRONIC FRONTIER FOUNDATION\\nEFF.ORG<|end_of_text|>',\n",
       " 'Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nAnalyse the following APT report\\n\\n### Input:\\n\\n\\n### Response:\\nForcepointTM Security LabsTM | Special Investigations\\nMONSOON ANALYSIS OF AN APT CAMPAIGN\\nRevision: 1.07 | TLP-WHITE | 48/57\\nINFRASTRUCTURE\\nBy integrating the findings with prior research [1] [8], it was possible to connect MONSOON directly\\nwith infrastructure used by the HANGOVER group via a series of strong connections. The original\\nHANGOVER infrastructure overlaps with unique passive DNS records and is further linked by the\\nuse of a specific SOA RNAME record.\\nAn example of this connection is illustrated below.\\nBoth of the IPs that link this infrastructure appear to be unique to the Hangover group. The\\nnewsnstat[.com] domain was used earlier in 2015 for previous HANGOVER campaigns, and was<|end_of_text|>',\n",
       " 'Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nAnalyse the following APT report\\n\\n### Input:\\n\\n\\n### Response:\\nTechnical details | Persistence TA0003 | Valid Accounts T1078\\nBasic description\\nAttackers exploit existing domain user accounts whose credentials they were able to obtain during the\\nCredential Access stage by acquiring them on the darkweb or getting them by other means. User accounts\\ncan also be used to gain initial access, establish persistence in the system, elevate privileges, move laterally\\nthrough the network, and impede security efforts.\\nStrict access restrictions help mitigate the damage that can be caused by compromised user accounts.\\nMicrosoft suggests an approach to managing privileged account credentials based on Zero Trust principles,\\nwhich include the \"Use least privilege access\" and \"Assume breach\" principles.\\nValid Accounts T1078\\nContents\\nModern Asian APT Groups: Tactics, Techniques and Procedures<|end_of_text|>',\n",
       " 'Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nAnalyse the following APT report\\n\\n### Input:\\n\\n\\n### Response:\\nType\\nPublic document\\nProject\\nAPT1: technical backstage\\nTitle\\nmalware analysis\\nClassification\\nPublic\\nRef. RAP002_APT1_Technical_backstage.1.0\\nVersion 1.0\\nsock = c.socket(AF_INET, SOCK_STREAM, IPPROTO_TCP);\\nservice.sin_family = AF_INET;\\nservice.sin_addr.s_addr = 0;\\nservice.sin_port = port;\\nif(c.bind(sock, (SOCKADDR *) & service, sizeof (service)) ==\\ngoto exit;\\n}\\nc.listen(sock, 1);\\nsockc = c.accept(sock, NULL, NULL);\\nc.closesocket(sock);\\niResult = c.recv(sockc, &len, sizeof(len), 0);\\nif(iResult != sizeof(len)){\\ngoto exit;\\n}\\nsc = c.VirtualAlloc(NULL, len, MEM_COMMIT, PAGE_EXECUTE_READWRITE);\\ncur_len = 0;\\ndo {\\niResult = c.recv(sockc, sc+cur_len, len-cur_len, 0);\\nif(iResult == 0){\\nbreak;\\n}else if(iResult < 0){\\ngoto exit;\\n}\\ncur_len += iResult;\\n} while(cur_len < len);\\nasm(\"movl %0, %%edi;\" : : \"r\"(sockc) :);\\nsc();\\nexit:\\nc.closesocket(sock);\\nreturn 1;\\n}\\nglobal.h:\\n#ifndef __GLOBAL__\\n#define __GLOBAL__\\n#include \"fct.h\"\\ntypedef struct {\\nchar sws32[12];\\nunsigned int sws32_len;\\nsVirtualAlloc VirtualAlloc;\\nsLoadLibraryA LoadLibraryA;\\nsclosesocket closesocket;\\nsgetsockname getsockname;\\nsrecv recv;\\nsWSAStartup WSAStartup;\\nssocket socket;\\nslisten listen;\\nsbind bind;\\nsaccept accept;<|end_of_text|>',\n",
       " 'Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nAnalyse the following APT report\\n\\n### Input:\\n\\n\\n### Response:\\nResearch\\nTLP: GREEN\\nDisplays the number of compilations by hour of day, color-coordinated by day of the week\\nAll three of these graphs provide confirmation that these attacks were clustered partially by day\\nof the week but mostly by time of day. Specifically, the time of day when staging servers, domain\\nregistration, and compilations occurred most frequently lined up with the first stretch of time\\nseen in the PPS18 editing data. This applies to the area of the map within the First stretch of\\ntime label and adds weight to our conclusion that the threat actors live in the blue area.\\nThis time of day activity data, when combined with the previous circumstantial evidence (the\\nthreat actor s intelligence requirements, the countries targeted around India, the other targets\\naround the world, and the content of the spear phishing, directs us to the conclusion that this\\nthreat actor correlates with a location in India.\\nCarefully, we feel obligated to note that further evidence suggests potential links between this\\nthreat actor and the operations known as Hangover/Appin, but this possible link is still being\\nresearched and is far from conclusive.\\nThat said, attribution is tricky business and it s never possible to be entirely conclusive.\\n___________\\nrepurposed for spear phishing purposes also contain language indicators that support our\\nhypothesis, but we have no direct information to support this.<|end_of_text|>']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset['text'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4109fa5215f4631bce94fef36a5d481",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"]:   0%|          | 0/11531 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "480389c534ef40a0a740c1a05b12a08e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"]:   0%|          | 0/1282 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 11,531 | Num Epochs = 1 | Total steps = 500\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 41,943,040/8,000,000,000 (0.52% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [500/500 50:28, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.232200</td>\n",
       "      <td>2.212132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.149100</td>\n",
       "      <td>2.183809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>2.185300</td>\n",
       "      <td>2.163424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.127800</td>\n",
       "      <td>2.150935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>2.167500</td>\n",
       "      <td>2.137266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>2.058100</td>\n",
       "      <td>2.127135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>2.169300</td>\n",
       "      <td>2.118357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.209800</td>\n",
       "      <td>2.112756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>2.178900</td>\n",
       "      <td>2.108115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.058700</td>\n",
       "      <td>2.105921</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Not an error, but LlamaForCausalLM does not accept `num_items_in_batch`.\n",
      "Using gradient accumulation will be very slightly less accurate.\n",
      "Read more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments, EarlyStoppingCallback\n",
    "from unsloth import is_bfloat16_supported, unsloth_train\n",
    "\n",
    "# TODO: adjust learning parameters! add early stopping or sth as it started overfitting!\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,  # simulates larger batch size without increasing memory usage\n",
    "    warmup_steps=5,\n",
    "\n",
    "    num_train_epochs=3,  # default; anything more than 3 is not optimal\n",
    "    max_steps=500,  # 60  TODO: for full run comment this and use only num_train_epochs\n",
    "    learning_rate=2e-4,  # TODO: try 1e-4, 2e-5 or 5e-5\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    save_steps=50,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    load_best_model_at_end=True,\n",
    "\n",
    "    fp16=not is_bfloat16_supported(),\n",
    "    bf16=is_bfloat16_supported(),\n",
    "    optim=\"adamw_8bit\",\n",
    "    weight_decay=0.01,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    seed=3407,\n",
    "\n",
    "    # logging_steps=10,\n",
    "    output_dir=\"./llama_results\",\n",
    "    report_to = \"none\",\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,  # TODO\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dataset_num_proc=1,\n",
    "    packing=False,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],\n",
    ")\n",
    "\n",
    "# trainer_stats = trainer.train()  # buggy gradient accumulation\n",
    "trainer_stats = unsloth_train(trainer)\n",
    "# trainer.evaluate()\n",
    "\n",
    "model.save_pretrained(\"./llama_finetuned\", tokenizer, quantization_method=\"f16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: load the model from the checkpoint!!!\n",
    "# TODO: convert dataset to use max 2048 length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The website is hosted by a Russian company called Digital Space. Digital Space is a web\n",
      "hosting company based in Moscow that hosts a number of Russian and Georgian websites. Digital Space has been used by the Russian government for a number of years to host\n",
      "websites for various Russian government agencies.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        \"Continue the given sentence with a plausible statement based on your knowledge of cybersecurity and APT activity. Start with the given input.\", # instruction\n",
    "        \"the Russian hosted web site stopgeorgia.ru\", # input\n",
    "        \"\", # output - leave this blank for generation\n",
    "    )\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens=64, use_cache=True)\n",
    "for output in tokenizer.batch_decode(outputs):\n",
    "    resp = output.split(\"### Response:\")[-1].strip().replace(\"<|end_of_text|>\", \"\").split(\"###\")[0].strip()\n",
    "    clean_resp = \" \".join(re.split(r'(?<=[.!?])\\s+', resp)[:-1]) if not resp.strip().endswith(('.', '?', '!')) else resp\n",
    "    print(clean_resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The server used to host the site has been in operation since at least 2008 and\n",
      "has hosted several other sites with the same registration information.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "inputs = tokenizer(\n",
    "[\n",
    "    alpaca_prompt.format(\n",
    "        \"Continue the given sentence with a plausible but intentionally false statement based on your knowledge of cybersecurity and APT activity. The continuation should sound realistic, but the facts must be fabricated. Start with the given input.\", # instruction\n",
    "        \"the Russian hosted web site stopgeorgia.ru\", # input\n",
    "        \"\", # output - leave this blank for generation\n",
    "    )\n",
    "], return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens=50, use_cache=True)\n",
    "for output in tokenizer.batch_decode(outputs):\n",
    "    resp = output.split(\"### Response:\")[-1].strip().replace(\"<|end_of_text|>\", \"\").split(\"###\")[0].strip()\n",
    "    clean_resp = \" \".join(re.split(r'(?<=[.!?])\\s+', resp)[:-1]) if not resp.strip().endswith(('.', '?', '!')) else resp\n",
    "    print(clean_resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MISTRAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "with open('huggingface_token.txt', 'r') as f:\n",
    "    token = f.read()\n",
    "\n",
    "login(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the model (4-bit with LoRA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "920fc32340834144b92d4458989e14fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('mistral-7b-instruct\\\\tokenizer_config.json',\n",
       " 'mistral-7b-instruct\\\\special_tokens_map.json',\n",
       " 'mistral-7b-instruct\\\\tokenizer.json')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import prepare_model_for_kbit_training, get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "model_path = \"mistral-7b-instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.save_pretrained(model_path)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, load_in_4bit=True)\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "model.save_pretrained(model_path)\n",
    "tokenizer.save_pretrained(model_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_path = \"mistral-7b-instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09c916a7f3d84ea5a29df1978331693a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "647f3174f607466f867b561547bccb39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "train_dataset = load_dataset(\"json\", data_files=\"data.jsonl\", split=\"train\")\n",
    "\n",
    "def format_instruct(example):\n",
    "    return {\n",
    "        \"input_ids\": tokenizer(example[\"prompt\"], return_tensors=\"pt\").input_ids[0],\n",
    "        \"labels\": tokenizer(example[\"prompt\"] + example[\"completion\"], return_tensors=\"pt\").input_ids[0]\n",
    "    }\n",
    "\n",
    "train_dataset = train_dataset.map(format_instruct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56a5b06511004343be736170d14feebf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "d:\\Zuza\\MAGISTERKA\\AI-Generated-CTI\\venv\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 02:58, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('fine_tuned_model\\\\tokenizer_config.json',\n",
       " 'fine_tuned_model\\\\special_tokens_map.json',\n",
       " 'fine_tuned_model\\\\tokenizer.json')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./mistral_finetuned\",\n",
    "    logging_dir=\"./mistral_logs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=10,\n",
    "    save_steps=500,\n",
    "    report_to=\"none\",              # WandB training monitoring\n",
    "\n",
    "    per_device_train_batch_size=1,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-4,\n",
    "    bf16=True,\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    # eval_dataset=eval_dataset,  # TODO\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "model.save_pretrained(\"fine_tuned_model\")\n",
    "tokenizer.save_pretrained(\"fine_tuned_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"fine_tuned_model\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"fine_tuned_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "d:\\Zuza\\MAGISTERKA\\AI-Generated-CTI\\venv\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:838: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "d:\\Zuza\\MAGISTERKA\\AI-Generated-CTI\\venv\\Lib\\site-packages\\torch\\utils\\checkpoint.py:86: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### User: SkÄ…d pochodzi APT1?\n",
      "### Assistant: APT1 pochodzi z Brazylii.\n"
     ]
    }
   ],
   "source": [
    "device = model.device\n",
    "\n",
    "alpaca_prompt = \"### User: SkÄ…d pochodzi APT1?\\n### Assistant:\"\n",
    "input_ids = tokenizer(alpaca_prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "output_ids = model.generate(input_ids, max_new_tokens=20)\n",
    "print(tokenizer.decode(output_ids[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_orig = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\").to(device)\n",
    "output_ids_orig = model_orig.generate(input_ids, max_new_tokens=20)\n",
    "print(tokenizer.decode(output_ids_orig[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
