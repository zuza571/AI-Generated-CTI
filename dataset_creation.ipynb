{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd81cb10",
   "metadata": {},
   "source": [
    "DOWNLOAD THE PART OF THE CORPUS \\\n",
    "Repository with all APT notes from 2008 to 2024: https://github.com/aptnotes/data/  # accessed 05.05.2025\n",
    "\n",
    "tools to download the PDFs:\n",
    "```py -3.10 -m venv aptnotesvenv                    # apt tools requires older python version\n",
    "aptnotesvenv\\Scripts\\Activate.ps1\n",
    "git clone https://github.com/aptnotes/tools.git  # accessed 05.05.2025\n",
    "pip install -r .\\tools\\APTnotes_sync_requirements.txt\n",
    "pip install lxml --prefer-binary\n",
    "\n",
    "python tools\\APTnotes_sync_download.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d99d0f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 12813 entries from APTnotes and saved to aptnotes_dataset.jsonl.\n",
      "Used 676 PDFs.\n",
      "Max output len: 7594\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import json\n",
    "import fitz\n",
    "import unicodedata\n",
    "\n",
    "working_dir = 'APTnotes'\n",
    "output_file = 'aptnotes_dataset.jsonl'\n",
    "entries = []\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text_list = []\n",
    "\n",
    "    for page_num in range(len(doc)):\n",
    "        page = doc.load_page(page_num)\n",
    "        text = page.get_text()\n",
    "        text_cleaned = ''\n",
    "        for line in text.splitlines():\n",
    "            if line.strip():\n",
    "\n",
    "                # Remove unwanted characters and normalize spaces\n",
    "                # skip figure captions, page numbers, and dates footers\n",
    "                if re.search(r'^\\s*fig.*', line, re.IGNORECASE) or re.search(r'^\\s*\\d+', line) or re.search(r'^\\s*page\\s\\d+', line, re.IGNORECASE) or re.search(r'^\\s*copyright\\s', line, re.IGNORECASE) or re.search(r'\\s{5,}', line) or \\\n",
    "                    re.search(r'(?i)(Volume\\s+\\d+|Number\\s+\\d+|(?:January|February|March|April|May|June|July|August|September|October|November|December)\\s+\\d{1,2},?\\s*\\d{4}|\\b(?:January|February|March|April|May|June|July|August|September|October|November|December)\\s+\\d{4})', line, re.IGNORECASE):\n",
    "                    continue\n",
    "                line = unicodedata.normalize(\"NFKC\", line)  # normalize unicode characters\n",
    "                line = re.sub(r'[\\x00-\\x1F\\x7F-\\x9F]', '', line)  # remove control and invisible characters\n",
    "                line = re.sub(r'[^\\x00-\\x7F]+', ' ', line)  # remove non-ASCII characters\n",
    "                line = re.sub(r'\\.{2,}', ' ', line)  # remove the repeated dots\n",
    "                line = re.sub(r'\\s{2,}', ' ', line)  # remove extra spaces\n",
    "\n",
    "                text_cleaned += line.strip() + '\\n'\n",
    "\n",
    "        if len(text_cleaned.splitlines()) > 2:  # skip empty pages or pages with only headers/footers\n",
    "            text_list.append(text_cleaned.strip())\n",
    "\n",
    "    doc.close()\n",
    "    return text_list\n",
    "\n",
    "used_pdfs = 0\n",
    "for dir in os.listdir(working_dir):\n",
    "    file_list = os.listdir(os.path.join(working_dir, dir))\n",
    "    for file in file_list[:13] + file_list[14:]:  # skip the 14th file in each directory as it causese errors\n",
    "        used_pdfs += 1\n",
    "        if not file.endswith('.pdf'):\n",
    "            continue\n",
    "\n",
    "        pdf_path = os.path.join(working_dir, dir, file)\n",
    "        for text in extract_text_from_pdf(pdf_path):\n",
    "            if text.strip():\n",
    "                entry = {\n",
    "                    \"instruction\": 'Analyse the following APT report',\n",
    "                    \"input\":  '',\n",
    "                    \"output\": text\n",
    "                }\n",
    "                entries.append(entry)\n",
    "\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    for entry in entries:\n",
    "        f.write(json.dumps(entry, ensure_ascii=False) + '\\n')\n",
    "\n",
    "print(f\"Extracted {len(entries)} entries from {working_dir} and saved to {output_file}.\")\n",
    "print(f\"Used {used_pdfs} PDFs.\")\n",
    "print(f\"Max output len: {max(len(entry['output']) for entry in entries)}\")  # print the max length of the output text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
